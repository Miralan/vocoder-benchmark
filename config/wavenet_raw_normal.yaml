dataset:
  batch_size: 64
  frames_per_clip: 3
  clips_per_utterance: 10
  padding_frames: 2
  dataloader_num_workers: 4
  dataloader_prefetch_factor: 4

model:
  quantize_channels: 256  # 65536 or 256
  # This should equal to `quantize_channels` if mu-law quantize enabled
  # otherwise num_mixture * 3 (pi, mean, log_scale)
  # single mixture case: 2
  out_channels: 30
  layers: 24
  stacks: 4
  residual_channels: 128
  gate_channels: 256
  skip_out_channels: 128
  cin_channels: 80
  gin_channels: 1
  n_speakers: 7
  dropout: 0.0
  kernel_size: 3
  cin_pad: 2
  upsample_conditional_features: True
  # Input type:
  # 1. raw [-1, 1]
  # 2. mulaw [-1, 1]
  # 3. mulaw-quantize [0, mu]
  # If input_type is raw or mulaw, network assumes scalar input and
  # discretized mixture of logistic distributions output, otherwise one-hot
  # input and softmax output are assumed.
  input_type: "raw"
  output_distribution: "Normal" # Logistic or Normal
  n_iterations: 1000000
  learning_rate: 1.0e-4
  upsample_params:
    upsample_scales: [4, 4, 5, 3] # should np.prod(upsample_scales) == hop_size
    cin_channels: 80
    cin_pad: 2
